Rough outline of macro details:

    Environment:
    The simplest version of the environment consists of a 2d grid in which each square
    of the grid can be inhabited by nothing, an agent, or a piece of food. The agents
    have a continuously declining health bar, and they die when health reaches zero.
    Eating food increases health. In this very simple environment, the agents only
    need to learn to move towards and consume food, which shouldn't be too difficult.
    In a slightly more difficult version, we can also create lava squares, which will
    injure or kill an agent upon entering. By placing food in certain pockets with many
    lava squares, the agents will need to learn to avoid lava and find food. Still a
    very simple problem that requires no coordination with other agents. The simplest
    environmental obstacle that would require some degree of coordination is a payout
    structure for food that benefits agents who share food, such as in the games
    'harvest' or 'cleanup' which are used in the social influence paper. These games
    still don't require a sophisticated degree of communication, just an ability to
    see past the greedy motivations and engage in socially responsible behavior. A
    natural next step past this benchmark and into the territory of communication is
    the inclusion of tasks which explicity require communication, such as synchronized
    button pushing or delivery of a piece of token information. We should start with
    the simplest version and move our way up.

    Actions:
    Movement (left, right, up, down, none)
    Speech (real valued vector)
    Consume (yes, no)

    Input at each timestep:
    Sight
    Audio
    Health

    At each timestep, sight is passed into a convolutional network and
    audio is passed into a multi-head attention (?) network. The individual outputs
    of these networks are then concatenated with the agent's health, and the
    result is passed into a recurrent-ish (LSTM?) network which then outputs scores
    for each action (this score vector will be of length 7 + n, where n is the length
    of the speech vector), which determines the actions of the agent. These 3 networks
    are then trained using an undecided RL algorithm to maximize the reward function.

    Reward function is implemented as a (feedforward or recurrent) network which takes
    as input the previous observations (sight, audio, health before action, health
    after action), and action, and outputs a real number.
    The reward function is fixed during an agents lifetime, and is initialized for
    each agent as some mixture of the reward functions of that agent's parents. The
    first generation has randomly initialized reward functions.

    TODO: Should we make them choose between moving or eating at a given timestep, 
    or allow them to do both?

#=====================================================================================
Things to come back to:

    Make ``self.grid`` a numpy array instead of a ``Dict`` with ``Tuple`` keys. 

    Considering the order in which agents make decisions, dealing with collisions 
    between the actions of two agents during environment steps.  

    Instead of ``self.rows``, ``self.cols`` and whatever, we just have a 
    ``self.env_config`` and pass it to the rest of the class and call variables using 
    string keys.

    Make Env object less monolithic. Break it up into environment functionality (_move
    and _consume, for example) and running functionality (get_action_dict).

    Unique identifiers for an unbounded number of agents. For now, an agent's observation
    representaiton is a one hot vector for the object type identifier together with a
    one hot vector that distinguishes that agent from all other agents. This only works
    when the total number of agents is static. We will keep it this way for now but this
    needs to change to have agents die and new ones born. Maybe use agent DNA as the unique
    identifier. 

    Something to consider: Should empty squares be given their own one-hot encoding or should
    those appear as all zeros?

    In env.py, self.grid is implemented as a 4 dimensional numpy array. This is bad because it
    makes it impossible to process without flattening it or embedding it or something. Right
    now our solution is to make agents mask food (if an agent is standing on top of a piece of
    food, it's invisible to other agents) to make the observation into a 3 dimensional array.
    This is not a permanent solution though, and we need to change this eventually.

    Instead of going with the above paragraph, what we are instead going to do is get rid of
    instance identifiers entirely, so that agents are indistinguishable from other agents by
    sight, food is indistinguishable from other food, etc. 

    In env.py, we set self.dones (which is initialized as a set) to the done dictionary in
    step(). This doesn't seem to cause any problems right now, but we also don't know if
    self.dones is needed for anything external.

    Question: is the old policy being overwritten somehow by ``rllib``? Why is the call
    to ``self.policy.get_action`` still in ``agent.py``?

    Initially we were unsure about including the health bar as an input to the reward
    function, and about whether or not we should include the difference in health over
    time versus just the health from multiple timesteps. If we chose to only include
    the health at the current timestep, the reward function would not be able to tell
    any information about the changes in health if it is not recurrent. We ended up
    deciding to use give the reward function the health before an action and the health
    after the action was taken.

        t-1        t
    |---------|---------|
     *'  %'  &' *  %   & 
    ' denotes previous timestep (t-1).
    '' denotes (t-2).  

    Reward function should take as input an observation, and the action that resulted 
    from that observation.
 
    *': Policy executes action within timestep t-1 at *' from inputs collected at &''.
    %': Reward computed using observations from timestep t-2 at &'' and action from 
        timestep t-1 at *'.
    &': Inputs recorded, used as input to policy at *.

    *': Policy executes action within timestep t at * from inputs collected at &'.
    %': Reward computed using observations from timestep t-1 at &' and action from 
        timestep t at *.
    &': Inputs recorded, used as input to policy at (t+1).

    Should we eventually make it so that reward functions change during an agent lifetime?

    Remove the action constants from the settings file, place them in a constants file
    which is not passed in or edited by the user, it's just read by the rest of the program.
    
    Convert to package with __init__.py's?

    Mating should be considered an action, so that a reward function should learn to
    incentivize mating, which reflects real life. Also, we are going to hard code 2 genders
    for awhile, but whether or not the reproductive process is itself dependent on DNA is
    an interesting discussion for later (recursive control of DNA).

    At some point we should consider replacing just the health bar with a more descriptive/
    complex/granular bodily state, probably comprised of multiple resources (energy, fitness,
    , ?).

    Mating is currently executed after moving, though that is kind of odd because then
    agents have to choose to mate before they are in position to mate. This should
    change eventually.

    We should consider making ``self.agents`` a dict instead of a list, so that we don't
    have a list of unbounded size full of dead bees.

    Make options for different types of crossover and mutation.

    We want to tune hyperparameters for population stability, which means that we want to
    optimize the hyperparameters to maximize some objective that is a combination of episode
    length, average agent lifetime, saturation of environment with agents, any more?

    Instead of having a minimum health threshold for mating, we can implement a mechanism
    where when mating, parents choose an amount of health to lose/give to the child. So
    when pating, parents choose x health, they lose that amount, and the kid gains that
    amount (starting from 0). If other parent didn't choose to mate, then they do not lose
    any health and the child only gets one parent's worth of health. We could also make
    the amount of health gaussian, not part of the policy output.

    We should split the single health bar into a health bar and a hunger bar, so that
    agents can't just live forever if they keep eating, and they will be more incentivized
    to have kids.

    For when we implement identity vectors: Identity should be represented by a randomly
    initialized vector (for the first generation). Identity DNA is passed along to children
    in the same way that reward DNA is. Once the agents have some way to negatively impact
    other agents, this will allow a way for agents to trust agents (those with similar
    identity DNA) and mistrust other agents (those with different identity DNA).

    Order:
    1. Zero to food (what we already have)
    2. Cooperation/social dilemmas (harvest/cleanup)
    3. Combat/groupish behavior (three rules from righteous mind + five other rules?)
    4. Communication
    5. Complexity (try to encourage geographic isolation and other catalysts for evolution)

    Idea for recursive control of DNA:
    DNA controls the replication process of DNA. So the replication process is
    defined as a function from two segments of DNA to one segment of DNA, and
    the function is represented by a fully connected, two layer network whose
    weights are the DNA values.

    Should we penalize trying to eat when there's no food there? Probably because
    otherwise they might as well eat all the time.

    Agents via reproduction should start with health that is dependent on parents' health 
    at time of birth, not full health.

    Create an object type for grid boundary, so that agents can distinguish between the
    edge of the grid and just empty space.

    Add agent age as input to reward function.

    Get rid of all of this array conversion for actions. We have three formats right now
    (5D multi binary, 3-tuple of ints, and torch.Tensor) and we should just have torch.Tensor.

    Should we consider initializing the first members of the population with DNA representing
    a naive reward function to avoid the population all dying immediately?

    Should the other return values of the ``Policy.act()`` function be tuples, or some sort
    of aggregate? For example value seems to be a float of some sort.

    Should the distribution over actions generated by the policy treat actions from different
    action subspaces independently? It does now.

    Should the action sampled from the distribution generated by the policy be used as input
    to the next timestep?

    Shape of ``actions`` (input to FixedCategorical.log_probs) is [num_processes, len(action_space)]
    where action_space is an instance of gym.spaces.Discrete.

    Reaching out to Joel/declaring paper as finished should be conditional on
    the system working on a fundamental level. It doesn't necessarily need to
    have great results, but each of the individual components need to at least
    do their job, e.g. agents should be able to learn policies which maximize
    their intrinsic reward functions.

    Apply to this for computing resources:
    (https://www.olcf.ornl.gov/for-users/documents-forms/olcf-directors-discretion-project-application/)

    Consider rounding health values in log and/or pickling logs instead of writing text.

    Consider making agents lose health faster the longer they are alive, so that old agents
    are not identical in terms of healthyness.

    # TODO LATER.
    - Add health bottleneck for mating
    - Lose more health for moving

    Right now, when saving and loading runs, we overwrite the old save, since saving a
    run takes about 5 gigs. In the future, we may want the ability to keep the
    intermediate save points so that we can make multiple branches from that point, in
    which case we'll need to add the functionality to save these intermediate steps.

    We moved the call of env._log_state (renamed env.log_state) to trainer.py instead of
    from env.step. This causes env.iteration to be 1 value larger at each timestep than
    it was earlier.

    Currently the agents aren't really able to learn to maximize their intrinsic reward
    functions. This may be due to optimization, lack of computing resources, or that the
    reward functions are just too hard to learn. To test the third option, we should make
    very simple reward functions (linear combinations of inputs) and see if the agents are
    able to learn to maximize them. Another possibility is to impose some condition on the
    weights of the reward network to make them "less non-linear", though this might be more
    time/math than its worth. We should also make some script which takes a reward function
    weights and provides some insight about what that function is actually rewarding. Also
    mess with hyperparameters, especially because depth and height of reward network
    contribute to nonlinearity. We should also look for details in the Wang paper "Evolving
    intrinsic motivations for altruistic behavior" about how they parameterize their reward
    networks, what optimization methods/resources they use for training, etc.

    Upon some reward network analysis, we found the following distributions for the rewards
    resulting from fixing a given action for a reward network of 1 vs. 3 layers:

    1:
    {
        (0, 0): {'mean': 1.6710889711187393, 'std': 2.6830863962492755},
        (0, 1): {'mean': 1.4405449986986656, 'std': 2.681561864293255},
        (0, 2): {'mean': 1.632757996828577, 'std': 2.663539715560427},
        (0, 3): {'mean': 0.9429008474412299, 'std': 2.6019059282551624},
        (0, 4): {'mean': 1.3787049708595864, 'std': 2.6407181281861125},
        (1, 0): {'mean': 1.557499144304561, 'std': 2.663676365281745},
        (1, 1): {'mean': 1.2947283264494556, 'std': 2.646747033807548},
        (2, 0): {'mean': 1.1658612818973182, 'std': 2.6167149303113257},
        (2, 1): {'mean': 1.6934441103251188, 'std': 2.703221366882886}
    }

    3:
    {
        (0, 0): {'mean': 25.94105866013453, 'std': 35.65950986852765},
        (0, 1): {'mean': 22.36563662856978, 'std': 36.28916437707366},
        (0, 2): {'mean': 30.44983135554303, 'std': 36.059172880909664},
        (0, 3): {'mean': 26.299393034271016, 'std': 36.16811218858602},
        (0, 4): {'mean': 29.91889629784399, 'std': 36.388935714111746},
        (1, 0): {'mean': 24.467416819677098, 'std': 36.05558641269532},
        (1, 1): {'mean': 29.77231193537858, 'std': 35.96699633133658},
        (2, 0): {'mean': 30.281839454767997, 'std': 36.57556147981293},
        (2, 1): {'mean': 23.011885372851495, 'std': 36.04396301184068}
    }

    The stark difference here might just be variance (we only looked at a single reward
    network of each type) but we should go look at more of them and investigate.

    A metric for determining whether or not agents are learning their reward functions:

        Compute the ground-truth distribution by iterating over the action
        space and computing the mean output of the reward network given each
        action tuple. These are treated as an unnormalized distribution, which
        we then normalize with a softmax. We then compute the actual, running,
        learned distribution by sampling from the action distribution taking
        during training by a given agent. In this, we treat frequency as the
        probability mass and again, take a softmax over all possible action
        inputs to normalize the distribution. We then compute the running
        KL-divergence of these two distributions to determine how well-trained
        the agent is.

    Reward network computes rewards for actions as a sum of rewards for
    subactions. This is a problem because this imposes the constraint that
    subaction rewards are independent.  We encountered a similar problem with
    the action distribution. Since the action distribution is a product of
    distributions over subactions, the agent can't choose distributions over
    actions which have dependencies between subactions.

    Learning rate scheduling should be better. Right now we anneal the learning
    rate linearly towards a minimum learning rate, which will be assumed when
    the agent has age 1.0 / config.aging_rate, which is the minimum agent
    lifetime.

    The distribution over the actions defined by the policy output exhibits
    strange behavior. First of all, the action distribution is uniform at the
    beginning of training, despite the fact that the policy network is randomly
    initialized. There are two main possibilities. The first is that there is a
    bug somewhere causing the value of the distribution to be reset, or
    something like that. The other is that the individual output values of the
    policy network are just too small (between 1e-3 and 1e-7 for CNNBase, and a
    bit larger for MLPBase), so that taking the softmax over the logits
    produces a near uniform distribution. The other strange thing about the
    policy network is that the outputs don't change very much as the input
    varies. This may be due to the fact that we observed this while the network
    was trained to optimize a reward function which only takes the action as input,
    meaning that the optimal policy will always output the same thing. Another
    possibility is the same as above, that the softmax'd outputs of the policy
    look very similar because the inputs to the softmax are too damn small.

    env._get_optimal_action_dists() can be made more efficient by parallelizing
    calls to agent.compute_reward() across actions and agents, then using torch
    operations to perform the softmax in parallel.

    Consider a constrained zero-to-food setting in which the food density is 100%
    and agents are guaranteed a health increase whenever they choose an action
    which includes ``EAT``. We will attempt to get the agents' reward networks to
    propagate towards the optimal action distribution in this setting, which is
    a uniform, maximum probability mass for each action which includes ``EAT`` and
    a zero probability mass for all others. We will compare the distribution given
    by the reward networks of the evolved agents to the optimal distribution using
    our existing KL divergence function, where the greedy temperature is set to
    ``1``, i.e. minimally greedy.

    Ideas for accelerating natural selection:
    - Increase aging rate with age (maybe with a delay?)
    - Disallow mating until maturity (threshold policy score, age, or health?)
    - Better adaptive food rate to control competition
    - Increase speed by trimming easy fat
      - Metric calculation at every step
      - Printing/writing to file at every step
      - Profile training process to inspect relative running time of phases

    At some point we should try different parameterizations of the reward network.
    There's no obvious reason why (at least for simple tasks) we couldn't use
    radial basis functions, fourier series, etc.

    The food score evaluation that we wrote is meaningful only when the inputs
    to the reward network are ["actions"]. If the observation is input then
    there doesn't seem to be a clear analogue.

    To not have a hard-coded policy score threshold, we can have the threshold be some
    fixed percentage of the mean policy score, where the mean policy score is computed
    over the space of randomly initialized policy networks and reward networks.

    Ideas for more general metrics:
    - Eating frequency
    - Mating frequency
    - Timesteps for policy to learn reward function

    Consider using the last hidden state of the policy network as the input to
    the reward network instead of the observation, and possibly instead of the
    action as well. Consider using a sophisticated recurrent policy like a
    transformer. Consider adding a memory-based model to try.

    Try to make the reward function smoother without sacrificing expressivity.

    Coming back to the repo for the first time in awhile, things to consider/remember:
    - Is it true that num_processes has to be 1 for anything to work? What
      would it mean for us to implement asynchronous training in a meaningful
      way here?
    - Our adaptive food regeneration is probably bad

    Idea for later: Make trainer less monolithic. It's impossible to test
    individual parts of the pipeline right now (like saving and loading)
    because it all happens in one function, and we can't test the values of the
    variables within this function. If we had a trainer class and keep track of
    the trainer state that way, then make individual functions for each part of
    the training pipeline, we could call each function and check the values of
    those pieces of state before and after each function call. Making trainer
    less monolithic will enable testing it.

#===========================================================================================

    October sprint:
    - Add docstrings and move rllib stuff into new branch (DONE)
    - Fix agent learning rate (DONE)
    - Fix observation CNN (DONE)
    - Add food regeneration dependency (DONE)
    - Add agent lifetime and other relevant stats to log (DONE)
    - Investigate agents not moving (DONE)
    - Formulate/run long experiment (DONE)
    - Refactor
      - Clean up hardcoding
      - Docstrings, mypy, blacken

   November sprint:
   - Bug with number of agents (DONE)
   - Implement adaptive food regrowth rate to keep population stable (DONE)
   - Change computation of avg agent lifetime (DONE? Currently EMA)
   - Logs (DONE)
   - Implement faster initialization by reusing policies of dead agents (DONE)
   - Faster reinitialization by loading from fixed set of randomly initialized state dicts (DONE)
   - Implement saving and loading runs (DONE)
     - Env saving and loading (DONE)
     - Policy saving and loading (DONE)
     - Saving and loading runs with keywords (DONE)
     - Work out settings between resuming runs (absolute vs. relative timesteps, changing settings) (DONE)

   December sprint:
   - Log analysis
     - Rewards over time (DONE)
     - Children per agent (DONE)
   - Reproductive mechanics
     - Health bottleneck (DONE)
     - Consentual reproduction (DONE)
   - Reward engineering
     - Linear reward function baseline (DONE)
     - Tweak hyperparameters (especially reward network parameters) (DONE?)
     - Reward network analysis to determine rewarded behavior (DONE?)

   January sprint:
   - Reward engineering
     - Compute policy score (DONE)
   - Write tests
   - Convert action space to discrete (DONE)
   - Clean up trainer.py
     - Move metric analysis into another file (DONE)
     - Fix printing, get analysis to work for all algorithms
     - Keep moving functionality out of trainer.py and into modular pieces.
   - Run experiments (zero to food)
     - Implement reward network evaluation (DONE)
     - Choose good hyperparameters
     - Run 'em (DONE)
   - Analyze results (DONE)
   - Refactor
     - Universal config file (DONE)
     - Move arguments from arguments.py into settings file (DONE)
     - Fix mixture of arguments (DONE)
     - Make arguments not positional
     - Minimize hardcoding
     - Clean up A2C code (DONE)
     - Make functionality more modular (separate training functionality and environment functionality)
     - Make all RNG depend on seed (DONE)
     - Add options for which metrics to compute/print?
   - Merge

#===========================================================================================

   February sprint:
   - Write tests
   - Add multiprocessing
     - Merge trainer versions (DONE)
       - Add loading to leader.py (DONE)
       - Clean up (DONE)
       - Make sure it runs okay (DONE)
     - Add profiling/timer
     - Multiprocess more functionality
   - Profile
     - Log profile statistics
     - Real time output
   - Run experiments
     - Experiment list (how many steps does it take to converge?):
       - 1 layer, action + observation, single agent
       - 1 layer, action + observation
       - 2 layers, action
       - 2 layers, action + observation, single agent
     - Run until convergence. If we can't get it to converge:
         - Vary batch size
         - Vary reward network complexity
         - Other reward network parameterizations
     - Better evaluation than policy score loss (normalized rewards?)
   - Refactor
     - Convert PPO implementation to collect rollouts in numpy arrays
     - Make arguments not positional
     - Remove food score and other functionality specific to non-observation reward functions (DONE)
     - Reorganize repo

   Other things:
   - Parallelize env.step function
   - Build agent sandbox
   - Rewrite PPO implementation
   - Garbage collection for agents in multi processed case
   - Record RAM usage
   - Speed up evolutionary process
     - Different mutation type
     - ?
   - Various reward network parameterizations
   - Evolutionary ideas:
     - Geographic speciation
     - Phenotypic variation
   - Add health to policy network input

#=========================================================================================

Future directions (6/8/2020)

Performance and cleanliness
---------------------------
- Clean up the RL code and use our own implementations
- Profiling + analyzing effect of optimization parameters (batch size)
- A3C-like training (multiple envs, where new agents appear in a designated position)
- Asynchronous stepping of (a single) environment
- Tie up loose ends in our code
  - Adaptive food regeneration
  - Resolve TODOs
  - Resolve settings loading behavior to match docstring of train()
  - Test saving and loading

Enable longer runs
------------------
- Work on cc to get > 8 hour training runs

Change optimization problem
---------------------------
- Lookup table for features like Jaderberg paper (DONE)
- Scale up complexity of reward networks over time
- Shared perception module between policy network and reward network (autoencoder)
- Brainstorm more inheritance than just reward network

Evaluation
----------
- Implement a way to evaluate population learning dynamics
- Better experiment management

New directions
--------------
- Bayesian optimization on policy network to try to get faster performance?
- AutoML Zero?


What we are doing now: Get longer runs to happen. This means that we need
saving and loading to work, then we need to augment this repo/cc with the
ability to perform a run over multiple nodes. First thing is just to get saving
and loading to work.

Fix bug where total rewards gets reset during continuation of saved run.
