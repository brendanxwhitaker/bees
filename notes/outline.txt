Rough outline of macro details:

    Environment:
    The simplest version of the environment consists of a 2d grid in which each square
    of the grid can be inhabited by nothing, an agent, or a piece of food. The agents
    have a continuously declining health bar, and they die when health reaches zero.
    Eating food increases health. In this very simple environment, the agents only
    need to learn to move towards and consume food, which shouldn't be too difficult.
    In a slightly more difficult version, we can also create lava squares, which will
    injure or kill an agent upon entering. By placing food in certain pockets with many
    lava squares, the agents will need to learn to avoid lava and find food. Still a
    very simple problem that requires no coordination with other agents. The simplest
    environmental obstacle that would require some degree of coordination is a payout
    structure for food that benefits agents who share food, such as in the games
    'harvest' or 'cleanup' which are used in the social influence paper. These games
    still don't require a sophisticated degree of communication, just an ability to
    see past the greedy motivations and engage in socially responsible behavior. A
    natural next step past this benchmark and into the territory of communication is
    the inclusion of tasks which explicity require communication, such as synchronized
    button pushing or delivery of a piece of token information. We should start with
    the simplest version and move our way up.

    Actions:
    Movement (left, right, up, down, none)
    Speech (real valued vector)
    Consume (yes, no)

    Input at each timestep:
    Sight
    Audio
    Health

    At each timestep, sight is passed into a convolutional network and
    audio is passed into a multi-head attention (?) network. The individual outputs
    of these networks are then concatenated with the agent's health, and the
    result is passed into a recurrent-ish (LSTM?) network which then outputs scores
    for each action (this score vector will be of length 7 + n, where n is the length
    of the speech vector), which determines the actions of the agent. These 3 networks
    are then trained using an undecided RL algorithm to maximize the reward function.

    Reward function is implemented as a (feedforward or recurrent) network which takes
    as input the previous observations (sight, audio, health before action, health
    after action), and action, and outputs a real number.
    The reward function is fixed during an agents lifetime, and is initialized for
    each agent as some mixture of the reward functions of that agent's parents. The
    first generation has randomly initialized reward functions.

    TODO: Should we make them choose between moving or eating at a given timestep, 
    or allow them to do both?

#=====================================================================================
Things to come back to:

    Make ``self.grid`` a numpy array instead of a ``Dict`` with ``Tuple`` keys. 

    Considering the order in which agents make decisions, dealing with collisions 
    between the actions of two agents during environment steps.  

    Instead of ``self.rows``, ``self.cols`` and whatever, we just have a 
    ``self.env_config`` and pass it to the rest of the class and call variables using 
    string keys.

    Make Env object less monolithic. Break it up into environment functionality (_move
    and _consume, for example) and running functionality (get_action_dict).

    Unique identifiers for an unbounded number of agents. For now, an agent's observation
    representaiton is a one hot vector for the object type identifier together with a
    one hot vector that distinguishes that agent from all other agents. This only works
    when the total number of agents is static. We will keep it this way for now but this
    needs to change to have agents die and new ones born. Maybe use agent DNA as the unique
    identifier. 

    Something to consider: Should empty squares be given their own one-hot encoding or should
    those appear as all zeros?

    In env.py, self.grid is implemented as a 4 dimensional numpy array. This is bad because it
    makes it impossible to process without flattening it or embedding it or something. Right
    now our solution is to make agents mask food (if an agent is standing on top of a piece of
    food, it's invisible to other agents) to make the observation into a 3 dimensional array.
    This is not a permanent solution though, and we need to change this eventually.

    Instead of going with the above paragraph, what we are instead going to do is get rid of
    instance identifiers entirely, so that agents are indistinguishable from other agents by
    sight, food is indistinguishable from other food, etc. 

    In env.py, we set self.dones (which is initialized as a set) to the done dictionary in
    step(). This doesn't seem to cause any problems right now, but we also don't know if
    self.dones is needed for anything external.

    Question: is the old policy being overwritten somehow by ``rllib``? Why is the call
    to ``self.policy.get_action`` still in ``agent.py``?

    Initially we were unsure about including the health bar as an input to the reward
    function, and about whether or not we should include the difference in health over
    time versus just the health from multiple timesteps. If we chose to only include
    the health at the current timestep, the reward function would not be able to tell
    any information about the changes in health if it is not recurrent. We ended up
    deciding to use give the reward function the health before an action and the health
    after the action was taken.

        t-1        t
    |---------|---------|
     *'  %'  &' *  %   & 
    ' denotes previous timestep (t-1).
    '' denotes (t-2).  

    Reward function should take as input an observation, and the action that resulted 
    from that observation.
 
    *': Policy executes action within timestep t-1 at *' from inputs collected at &''.
    %': Reward computed using observations from timestep t-2 at &'' and action from 
        timestep t-1 at *'.
    &': Inputs recorded, used as input to policy at *.

    *': Policy executes action within timestep t at * from inputs collected at &'.
    %': Reward computed using observations from timestep t-1 at &' and action from 
        timestep t at *.
    &': Inputs recorded, used as input to policy at (t+1).

    Should we eventually make it so that reward functions change during an agent lifetime?

    Remove the action constants from the settings file, place them in a constants file
    which is not passed in or edited by the user, it's just read by the rest of the program.
    
    Convert to package with __init__.py's?

    Mating should be considered an action, so that a reward function should learn to
    incentivize mating, which reflects real life. Also, we are going to hard code 2 genders
    for awhile, but whether or not the reproductive process is itself dependent on DNA is
    an interesting discussion for later (recursive control of DNA).

    At some point we should consider replacing just the health bar with a more descriptive/
    complex/granular bodily state, probably comprised of multiple resources (energy, fitness,
    , ?).

    Mating is currently executed after moving, though that is kind of odd because then
    agents have to choose to mate before they are in position to mate. This should
    change eventually.

    We should consider making ``self.agents`` a dict instead of a list, so that we don't
    have a list of unbounded size full of dead bees.

    Make options for different types of crossover and mutation.

    We want to tune hyperparameters for population stability, which means that we want to
    optimize the hyperparameters to maximize some objective that is a combination of episode
    length, average agent lifetime, saturation of environment with agents, any more?

    Instead of having a minimum health threshold for mating, we can implement a mechanism
    where when mating, parents choose an amount of health to lose/give to the child. So
    when pating, parents choose x health, they lose that amount, and the kid gains that
    amount (starting from 0). If other parent didn't choose to mate, then they do not lose
    any health and the child only gets one parent's worth of health. We could also make
    the amount of health gaussian, not part of the policy output.

    We should split the single health bar into a health bar and a hunger bar, so that
    agents can't just live forever if they keep eating, and they will be more incentivized
    to have kids.

    For when we implement identity vectors: Identity should be represented by a randomly
    initialized vector (for the first generation). Identity DNA is passed along to children
    in the same way that reward DNA is. Once the agents have some way to negatively impact
    other agents, this will allow a way for agents to trust agents (those with similar
    identity DNA) and mistrust other agents (those with different identity DNA).

    Order:
    1. Zero to food (what we already have)
    2. Cooperation/social dilemmas (harvest/cleanup)
    3. Combat/groupish behavior (three rules from righteous mind + five other rules?)
    4. Communication
    5. Complexity (try to encourage geographic isolation and other catalysts for evolution)

    Idea for recursive control of DNA:
    DNA controls the replication process of DNA. So the replication process is
    defined as a function from two segments of DNA to one segment of DNA, and
    the function is represented by a fully connected, two layer network whose
    weights are the DNA values.

    Should we penalize trying to eat when there's no food there? Probably because
    otherwise they might as well eat all the time.

    Agents via reproduction should start with health that is dependent on parents' health 
    at time of birth, not full health.

    Create an object type for grid boundary, so that agents can distinguish between the
    edge of the grid and just empty space.

    Add agent age as input to reward function.

    Get rid of all of this array conversion for actions. We have three formats right now
    (5D multi binary, 3-tuple of ints, and torch.Tensor) and we should just have torch.Tensor.

    Should we consider initializing the first members of the population with DNA representing
    a naive reward function to avoid the population all dying immediately?

    Should the other return values of the ``Policy.act()`` function be tuples, or some sort
    of aggregate? For example value seems to be a float of some sort.

    Should the distribution over actions generated by the policy treat actions from different
    action subspaces independently? It does now.

    Should the action sampled from the distribution generated by the policy be used as input
    to the next timestep?

    Shape of ``actions`` (input to FixedCategorical.log_probs) is [num_processes, len(action_space)]
    where action_space is an instance of gym.spaces.Discrete.

    Reaching out to Joel/declaring paper as finished should be conditional on
    the system working on a fundamental level. It doesn't necessarily need to
    have great results, but each of the individual components need to at least
    do their job, e.g. agents should be able to learn policies which maximize
    their intrinsic reward functions.

    Apply to this for computing resources:
    (https://www.olcf.ornl.gov/for-users/documents-forms/olcf-directors-discretion-project-application/)

    Consider rounding health values in log and/or pickling logs instead of writing text.

    Consider making agents lose health faster the longer they are alive, so that old agents
    are not identical in terms of healthyness.

    # TODO LATER.
    - Add health bottleneck for mating
    - Lose more health for moving

    Right now, when saving and loading runs, we overwrite the old save, since saving a
    run takes about 5 gigs. In the future, we may want the ability to keep the
    intermediate save points so that we can make multiple branches from that point, in
    which case we'll need to add the functionality to save these intermediate steps.

    We moved the call of env._log_state (renamed env.log_state) to trainer.py instead of
    from env.step. This causes env.iteration to be 1 value larger at each timestep than
    it was earlier.

    Currently the agents aren't really able to learn to maximize their intrinsic reward
    functions. This may be due to optimization, lack of computing resources, or that the
    reward functions are just too hard to learn. To test the third option, we should make
    very simple reward functions (linear combinations of inputs) and see if the agents are
    able to learn to maximize them. Another possibility is to impose some condition on the
    weights of the reward network to make them "less non-linear", though this might be more
    time/math than its worth. We should also make some script which takes a reward function
    weights and provides some insight about what that function is actually rewarding. Also
    mess with hyperparameters, especially because depth and height of reward network
    contribute to nonlinearity. We should also look for details in the Wang paper "Evolving
    intrinsic motivations for altruistic behavior" about how they parameterize their reward
    networks, what optimization methods/resources they use for training, etc.

    Upon some reward network analysis, we found the following distributions for the rewards
    resulting from fixing a given action for a reward network of 1 vs. 3 layers:

    1:
    {
        (0, 0): {'mean': 1.6710889711187393, 'std': 2.6830863962492755},
        (0, 1): {'mean': 1.4405449986986656, 'std': 2.681561864293255},
        (0, 2): {'mean': 1.632757996828577, 'std': 2.663539715560427},
        (0, 3): {'mean': 0.9429008474412299, 'std': 2.6019059282551624},
        (0, 4): {'mean': 1.3787049708595864, 'std': 2.6407181281861125},
        (1, 0): {'mean': 1.557499144304561, 'std': 2.663676365281745},
        (1, 1): {'mean': 1.2947283264494556, 'std': 2.646747033807548},
        (2, 0): {'mean': 1.1658612818973182, 'std': 2.6167149303113257},
        (2, 1): {'mean': 1.6934441103251188, 'std': 2.703221366882886}
    }

    3:
    {
        (0, 0): {'mean': 25.94105866013453, 'std': 35.65950986852765},
        (0, 1): {'mean': 22.36563662856978, 'std': 36.28916437707366},
        (0, 2): {'mean': 30.44983135554303, 'std': 36.059172880909664},
        (0, 3): {'mean': 26.299393034271016, 'std': 36.16811218858602},
        (0, 4): {'mean': 29.91889629784399, 'std': 36.388935714111746},
        (1, 0): {'mean': 24.467416819677098, 'std': 36.05558641269532},
        (1, 1): {'mean': 29.77231193537858, 'std': 35.96699633133658},
        (2, 0): {'mean': 30.281839454767997, 'std': 36.57556147981293},
        (2, 1): {'mean': 23.011885372851495, 'std': 36.04396301184068}
    }

    The stark difference here might just be variance (we only looked at a single reward
    network of each type) but we should go look at more of them and investigate.

    A metric for determining whether or not agents are learning their reward functions:

        Compute the ground-truth distribution by iterating over the action
        space and computing the mean output of the reward network given each
        action tuple. These are treated as an unnormalized distribution, which
        we then normalize with a softmax. We then compute the actual, running,
        learned distribution by sampling from the action distribution taking
        during training by a given agent. In this, we treat frequency as the
        probability mass and again, take a softmax over all possible action
        inputs to normalize the distribution. We then compute the running
        KL-divergence of these two distributions to determine how well-trained
        the agent is.


#===========================================================================================

    October sprint:
    - Add docstrings and move rllib stuff into new branch (DONE)
    - Fix agent learning rate (DONE)
    - Fix observation CNN (DONE)
    - Add food regeneration dependency (DONE)
    - Add agent lifetime and other relevant stats to log (DONE)
    - Investigate agents not moving (DONE)
    - Formulate/run long experiment (DONE)
    - Refactor
      - Clean up hardcoding
      - Docstrings, mypy, blacken

   November sprint:
   - Bug with number of agents (DONE)
   - Implement adaptive food regrowth rate to keep population stable (DONE)
   - Change computation of avg agent lifetime (DONE? Currently EMA)
   - Logs (DONE)
   - Implement faster initialization by reusing policies of dead agents (DONE)
   - Faster reinitialization by loading from fixed set of randomly initialized state dicts (DONE)
   - Implement saving and loading runs (DONE)
     - Env saving and loading (DONE)
     - Policy saving and loading (DONE)
     - Saving and loading runs with keywords (DONE)
     - Work out settings between resuming runs (absolute vs. relative timesteps, changing settings) (DONE)

   December sprint:
   - Log analysis
     - Rewards over time (DONE)
     - Children per agent (DONE)
   - Reproductive mechanics
     - Health bottleneck (DONE)
     - Consentual reproduction (DONE)
   - Reward engineering
     - Linear reward function baseline (DONE)
     - Tweak hyperparameters (especially reward network parameters) (DONE?)
     - Reward network analysis to determine rewarded behavior (DONE?)
     - Constrain network weights to produce "less non-linear" reward functions
     - Compute policy score
   - Run experiments
   - Analyze results
   - Refactor
     - Universal config file (DONE)
     - Move arguments from arguments.py into settings file
     - Fix mixture of arguments
     - Make arguments not positional
     - Minimize hardcoding
   - Merge

   Other things:
   - Parallelize env.step function
   - Build agent sandbox
   - Tests
   - Do away with minimum mating health
   - Some mechanic to make reproduction happen at later ages (minimum mating age?)
   - Run experiment with one agent, analyze learning curve to maximize intrinsic reward
   - Rewrite PPO implementation
